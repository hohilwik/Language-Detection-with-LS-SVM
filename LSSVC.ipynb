{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Least Squares Support Vector Classifier</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "\n",
    "2. [Using the classifier](#using_classifier)\n",
    "\n",
    "    2.1 [Different Kernels](#cpu_version)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Least Squares Support Vector Machine (LSSVM) is a variation of the original Support Vector Machine (SVM) in which we have a slight change in the objective and restriction functions that results in a big simplification of the optimization problem.\n",
    "\n",
    "First, let's see the optimization problem of an SVM:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + C \\sum_{i=1}^{n} \\xi_i &&\\\\\n",
    "    s.t. && y_i(\\vec{w}^T\\vec{x}_i+b)\\geq 1 - \\xi_i, && i = 1,..., n \\\\\n",
    "         && \\xi_i \\geq 0,                            && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case, we have a set of inequality restrictions and when solving the optimization problem by it's dual we find a discriminative function, adding the kernel trick, of the type:\n",
    "\n",
    "\n",
    "$$ f(\\vec{x}) = sign \\ \\Big( \\sum_{i=1}^{n} \\alpha_i^o y_i K(\\vec{x}_i,\\vec{x}) + b_o \\Big) $$\n",
    "\n",
    "Where $\\alpha_i^o$ and $b_o$ denote optimum values. Giving enough regularization (smaller values of $C$) we get a lot of $\\alpha_i^o$ nulls, resulting in a sparse model in which we only need to save the pairs $(\\vec{x}_i,y_i)$ which have the optimum dual variable not null. The vectors $\\vec{x}_i$ with not null $\\alpha_i^o$ are known as support vectors (SV).\n",
    "\n",
    "\n",
    "\n",
    "In the LSSVM case, we change the inequality restrictions to equality restrictions. As the $\\xi_i$ may be negative we square its values in the objective function:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + \\gamma \\frac{1}{2}\\sum_{i=1}^{n} \\xi_i^2 &&\\\\\n",
    "    s.t. && y_i(\\vec{w}^T\\vec{x}_i+b) = 1 - \\xi_i, && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "The dual of this optimization problem results in a system of linear equations, a set of Karush-Khun-Tucker (KKT) equations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "    0 & \\vec{d}^T \\\\\n",
    "    \\vec{y} & \\Omega + \\gamma^{-1} I \n",
    "\\end{bmatrix}\n",
    "\\\n",
    "\\begin{bmatrix} \n",
    "    b  \\\\\n",
    "    \\vec{\\alpha}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "    0 \\\\\n",
    "    \\vec{1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where, with the kernel trick, &nbsp; $\\Omega_{i,j} = y_i y_j K(\\vec{x}_i,\\vec{x}_j)$,  &nbsp;  $\\vec{y} = [y_1 \\ y_2 \\ ... \\ y_n]^T$, &nbsp; $\\vec{\\alpha} = [\\alpha_1 \\ \\alpha_2 \\ ... \\ \\alpha_n]^T$ &nbsp;  e &nbsp; $\\vec{1} = [1 \\ 1 \\ ... \\ 1]^T$.\n",
    "\n",
    "The discriminative function of the LSSVM has the same form of the SVM but the $\\alpha_i^o$ aren't usually null, resulting in a bigger model. The big advantage of the LSSVM is in finding it's parameters, which is reduced to solving the linear system of the type:\n",
    "\n",
    "$$ A\\vec{z} = \\vec{b} $$\n",
    "\n",
    "A well-known solution of the linear system is when we minimize the square of the residues, that can be written as the optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{z})=\\frac{1}{2}||A\\vec{z} - \\vec{b}||^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And have the analytical solution:\n",
    "\n",
    "$$ \\vec{z} = A^{\\dagger} \\vec{b} $$\n",
    "\n",
    "Where $A^{\\dagger}$ is the pseudo-inverse defined as:\n",
    "\n",
    "$$ A^{\\dagger} = (A^T A)^{-1} A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using the classifier <a class=\"anchor\" id=\"using_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from lssvm import LSSVC\n",
    "from utils.encoding import dummie2multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.509438652  0.490447973  0.459416789  0.389270137  0.389622357  \\\n",
      "0       0.511510     0.424546     0.388545     0.330314     0.317644   \n",
      "1       0.539133     0.463781     0.433143     0.367356     0.376333   \n",
      "2       0.513139     0.409610     0.436496     0.372693     0.319311   \n",
      "3       0.463976     0.435471     0.377037     0.360475     0.311384   \n",
      "4       0.476905     0.495385     0.394527     0.361846     0.340298   \n",
      "..           ...          ...          ...          ...          ...   \n",
      "647     0.448289     0.430851     0.372547     0.330591     0.289256   \n",
      "648     0.273047     0.226982     0.200011     0.198919     0.181098   \n",
      "649     0.587593     0.517364     0.430658     0.420942     0.383764   \n",
      "650     0.433647     0.379705     0.352394     0.298677     0.280193   \n",
      "651     0.403983     0.285895     0.311456     0.242800     0.212219   \n",
      "\n",
      "     0.364762973  0.288698906  0.302846864  0.266432168  0.253494728  ...  \\\n",
      "0       0.316382     0.266822     0.263619     0.221372     0.211483  ...   \n",
      "1       0.357649     0.281439     0.314662     0.274555     0.247000  ...   \n",
      "2       0.282797     0.266707     0.237778     0.219987     0.216142  ...   \n",
      "3       0.294565     0.258524     0.257626     0.219311     0.218076  ...   \n",
      "4       0.268012     0.279810     0.250259     0.209175     0.232238  ...   \n",
      "..           ...          ...          ...          ...          ...  ...   \n",
      "647     0.303341     0.281035     0.257243     0.218249     0.244647  ...   \n",
      "648     0.152846     0.129920     0.119654     0.123332     0.104463  ...   \n",
      "649     0.328383     0.350016     0.305537     0.283711     0.289225  ...   \n",
      "650     0.252168     0.227575     0.202606     0.190667     0.159354  ...   \n",
      "651     0.240038     0.193096     0.188437     0.164024     0.161220  ...   \n",
      "\n",
      "     0.150668575  0.123659178  0.120802395  0.113111912  0.09018767  \\\n",
      "0       0.127449     0.109630     0.108875     0.094885    0.093652   \n",
      "1       0.146518     0.113473     0.118289     0.105678    0.083247   \n",
      "2       0.135641     0.116761     0.101275     0.105786    0.086965   \n",
      "3       0.125542     0.122696     0.100640     0.106307    0.093149   \n",
      "4       0.116526     0.092465     0.094512     0.079958    0.071570   \n",
      "..           ...          ...          ...          ...         ...   \n",
      "647     0.168917     0.163596     0.149549     0.129925    0.136935   \n",
      "648     0.088959     0.097036     0.092295     0.093774    0.103922   \n",
      "649     0.204842     0.194134     0.182877     0.160149    0.158220   \n",
      "650     0.100898     0.090261     0.078006     0.076237    0.067067   \n",
      "651     0.097767     0.078540     0.090928     0.080834    0.060365   \n",
      "\n",
      "     0.090448952  0.070258121  0.066275418  0.062469762  1  \n",
      "0       0.077930     0.072631     0.072706     0.052948  1  \n",
      "1       0.086007     0.072391     0.067752     0.058821  1  \n",
      "2       0.082647     0.066378     0.067911     0.059040  1  \n",
      "3       0.088978     0.079133     0.065484     0.065666  1  \n",
      "4       0.062467     0.067109     0.051361     0.050336  1  \n",
      "..           ...          ...          ...          ... ..  \n",
      "647     0.123760     0.109373     0.110413     0.088084  7  \n",
      "648     0.089954     0.085009     0.068926     0.075627  7  \n",
      "649     0.147386     0.126668     0.127290     0.115942  7  \n",
      "650     0.054572     0.056909     0.053964     0.041353  7  \n",
      "651     0.074577     0.068797     0.059918     0.057094  7  \n",
      "\n",
      "[652 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./langfeatures.csv', sep=\",\")\n",
    "print(data)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_col = len(data.columns)\n",
    "num_row = len(data.index)\n",
    "\n",
    "normdata = data.iloc[:,0:num_col-1]\n",
    "\n",
    "# remove min, divide by variance\n",
    "for i in range(num_col-2):\n",
    "    x = data.iloc[:,i]\n",
    "    normdata.iloc[:,i] = (x-x.min())/(x.max()-x.min())\n",
    "#print(normdata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigval Not Sorted\n",
      "resort Sorted\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import numpy.linalg as linalg\n",
    "\n",
    "data_var = data.iloc[:,0:len(data.columns)-1]\n",
    "data_meanresidual = data_var-np.mean(data_var,axis=0)/np.std(data_var,axis=0)\n",
    "\n",
    "cov = np.cov(normdata, rowvar=False)\n",
    "\n",
    "eig_val, eig_vect = linalg.eig(cov)\n",
    "\n",
    "def checkSorted(arr):\n",
    "    n=len(arr)\n",
    "\n",
    "    if(n<2):\n",
    "        return True\n",
    "\n",
    "    flag = 1\n",
    "    for i in range(n-1):\n",
    "        if(arr[i]<=arr[i+1]):\n",
    "            flag=0\n",
    "    return flag\n",
    "\n",
    "if checkSorted(eig_val):\n",
    "    print(\"Sorted\")\n",
    "else:\n",
    "    print(\"Eigval Not Sorted\")\n",
    "\n",
    "id_sorted = np.argsort(eig_val)[::-1]\n",
    "\n",
    "eigval_sorted = eig_val[id_sorted]\n",
    "eigvect_sorted = eig_vect[:,id_sorted]\n",
    "\n",
    "if checkSorted(eigval_sorted):\n",
    "    print(\"resort Sorted\")\n",
    "else:\n",
    "    print(\"resort Eigval Not Sorted\")\n",
    "\n",
    "print(len(eig_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0.9998483241201501\n",
      "(652, 24)\n",
      "(652, 24)\n"
     ]
    }
   ],
   "source": [
    "n=6\n",
    "\n",
    "part_eigval = eigval_sorted[0:n]\n",
    "part_eigvect = eigvect_sorted[:,0:n]\n",
    "\n",
    "sqparteigval = part_eigval*part_eigval.transpose()\n",
    "sqeigval = eigval_sorted*eigval_sorted.transpose()\n",
    "\n",
    "p = np.sqrt(sqparteigval.sum())\n",
    "q = np.sqrt(sqeigval.sum())\n",
    "\n",
    "pca = p/q\n",
    "print(n)\n",
    "print(pca)\n",
    "    \n",
    "data_reduced = np.dot(part_eigvect.transpose(), \n",
    "normdata.transpose()).transpose()\n",
    "\n",
    "print(normdata.shape)\n",
    "print(data_meanresidual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_slice(ls, indexes):\n",
    "     ls_ = []\n",
    "     for i in indexes:\n",
    "        ls_.append(ls[i])\n",
    "     return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "frac = 60\n",
    "length = len(data_reduced)\n",
    "limit = math.floor(frac*length/100)\n",
    "\n",
    "Xtemp = data_reduced\n",
    "ytemp = data.iloc[:,len(data.columns)-1]\n",
    "\n",
    "X = np.array(Xtemp)\n",
    "y = np.array(ytemp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (391, 6)\n",
      "X_test.shape:  (261, 6)\n",
      "y_train.shape: (391,)\n",
      "y_test.shape:  (261,)\n",
      "np.unique(y_train): [1 2 3 4 5 6 7]\n",
      "np.unique(y_test):  [1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Import digits recognition dataset (from sklearn)\n",
    "# X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2020)\n",
    "\n",
    "# Scaling features (from sklearn)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_tr_norm = scaler.transform(X_train)\n",
    "X_ts_norm = scaler.transform(X_test)\n",
    "\n",
    "# Get information about input and outputs\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_test.shape:  {X_test.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_test.shape:  {y_test.shape}\")\n",
    "print(f\"np.unique(y_train): {np.unique(y_train)}\")\n",
    "print(f\"np.unique(y_test):  {np.unique(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Kernels <a class=\"anchor\" id=\"cpu_version\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian kernel:\n",
      "acc_test =  0.8467432950191571 \n",
      "\n",
      "Polynomial kernel:\n",
      "acc_test =  0.8544061302681992 \n",
      "\n",
      "Linear kernel:\n",
      "acc_test =  0.8735632183908046 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the classifier with different kernels\n",
    "\n",
    "print('Gaussian kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='rbf', sigma=.5) # Class instantiation\n",
    "lssvc.fit(X_tr_norm, y_train) # Fitting the model\n",
    "y_pred = lssvc.predict(X_ts_norm) # Making predictions with the trained model\n",
    "acc = accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)) # Calculate Accuracy\n",
    "print('acc_test = ', acc, '\\n')\n",
    "\n",
    "print('Polynomial kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='poly', d=2)\n",
    "lssvc.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc.predict(X_ts_norm)\n",
    "acc = accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred))\n",
    "print('acc_test = ', acc, '\\n')\n",
    "\n",
    "print('Linear kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='linear')\n",
    "lssvc.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc.predict(X_ts_norm)\n",
    "acc = accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred))\n",
    "print('acc_test = ', acc, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSSVC' object has no attribute 'predicterror'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m yprederr \u001b[39m=\u001b[39m lssvc\u001b[39m.\u001b[39;49mpredicterror(X_ts_norm)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(yprederr)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LSSVC' object has no attribute 'predicterror'"
     ]
    }
   ],
   "source": [
    "yprederr = lssvc.predicterror(X_ts_norm)\n",
    "print(yprederr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.8735632183908046\n",
      "acc_test =  0.8735632183908046\n"
     ]
    }
   ],
   "source": [
    "lssvc.dump('model')\n",
    "loaded_model = LSSVC.load('model')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(loaded_model.predict(X_ts_norm))\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2abdc3c7cf459b294aab3ec620a23d5aef773baf1b3a9f88de4931caf2d52f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
